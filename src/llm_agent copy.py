"""
LLM Agent with Tools

This module demonstrates how to create an LLM agent with custom tools using LangChain and LangGraph.
"""

from copy import copy
import os
import time
import logging
from datetime import datetime
from dotenv import load_dotenv, find_dotenv
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from toolscopy import retrieve_Fitting_Instructions
from pydantic import SecretStr
import openai
from langgraph.checkpoint.memory import InMemorySaver, MemorySaver
from langsmith import traceable
from langchain_core.messages import AIMessage, ToolMessage, BaseMessage
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from typing import Any, Dict, List, Optional
import json

load_dotenv(find_dotenv())

api_key = os.getenv("OPENAI_API_KEY")
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE")

if not api_key:
    print("âŒ OPENAI_API_KEY not loaded.")
    # TODO: Consider raising an error here instead of continuing
    # raise ValueError("OPENAI_API_KEY is required but not found")
else:
    print("âœ… OPENAI_API_KEY loaded.")

if not OPENAI_API_BASE:
    print("âš ï¸ OPENAI_API_BASE not found.")
else:
    print(f"âœ… OPENAI_API_BASE = {OPENAI_API_BASE}")


# ============================================================================
# LLM Debug Callback System
# ============================================================================

class DebugCallback(BaseCallbackHandler):
    """å®Œæ•´çš„ LLM Debug Callback ç³»ç»Ÿï¼Œæ•æ‰å¹¶æ‰“å°æ‰€æœ‰å†…éƒ¨æ¶ˆæ¯"""
    
    def __init__(self):
        super().__init__()
        self.message_counter = 0
        self.llm_call_counter = 0
        self.current_prompt = None
        self.raw_response_parts = []
    
    def _print_separator(self, title: str = ""):
        """æ‰“å°åˆ†éš”çº¿"""
        if title:
            print("\n" + "="*80)
            print(f"  {title}")
            print("="*80)
        else:
            print("-"*80)
    
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        """LLM è°ƒç”¨å¼€å§‹"""
        self.llm_call_counter += 1
        self.raw_response_parts = []
        
        self._print_separator(f"ğŸš€ LLM CALL #{self.llm_call_counter} - START")
        
        # æ‰“å° prompts
        print(f"\nğŸ“ PROMPTS ({len(prompts)} prompts):")
        for i, prompt in enumerate(prompts):
            print(f"\n  Prompt {i+1}:")
            print(f"  {prompt[:500]}{'...' if len(prompt) > 500 else ''}")
        
        # æ‰“å° kwargs ä¸­çš„æœ‰ç”¨ä¿¡æ¯
        if kwargs:
            print(f"\nğŸ”§ KWARGS:")
            for key, value in kwargs.items():
                if key not in ['run_id', 'parent_run_id', 'tags', 'metadata']:
                    print(f"  {key}: {value}")
        
        self.current_prompt = prompts[0] if prompts else None
    
    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """æ¥æ”¶åˆ°æ–° tokenï¼ˆæµå¼è¾“å‡ºï¼‰"""
        self.raw_response_parts.append(token)
    
    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """LLM è°ƒç”¨ç»“æŸ"""
        self._print_separator(f"âœ… LLM CALL #{self.llm_call_counter} - END")
        
        # æ‰“å° kwargs ä¸­çš„æœ‰ç”¨ä¿¡æ¯
        if kwargs:
            print(f"\nğŸ”§ KWARGS:")
            for key, value in kwargs.items():
                if key not in ['run_id', 'parent_run_id', 'tags', 'metadata']:
                    print(f"  {key}: {value}")
        
        # æ‰“å°åŸå§‹å“åº”
        print(f"\nğŸ“¦ RAW RESPONSE:")
        if response.llm_output:
            print(f"  llm_output: {json.dumps(response.llm_output, indent=2, ensure_ascii=False)}")
        
        # æ‰“å°æ‰€æœ‰ generations
        for i, generation_list in enumerate(response.generations):
            print(f"\n  Generation Batch {i+1}:")
            for j, generation in enumerate(generation_list):
                print(f"\n    Generation {j+1}:")
                
                # æ‰“å°æ¶ˆæ¯å¯¹è±¡
                if hasattr(generation, 'message'):
                    msg = generation.message
                    print(f"      Message Type: {type(msg).__name__}")
                    print(f"      Content: {str(msg.content)[:500]}")
                    
                    # æ£€æŸ¥ tool_calls
                    if hasattr(msg, 'tool_calls') and msg.tool_calls:
                        print(f"\n      ğŸ”§ TOOL_CALLS ({len(msg.tool_calls)}):")
                        for k, tc in enumerate(msg.tool_calls):
                            if isinstance(tc, dict):
                                print(f"        [{k}] {json.dumps(tc, indent=6, ensure_ascii=False)}")
                            else:
                                print(f"        [{k}] id={getattr(tc, 'id', 'N/A')}, "
                                      f"name={getattr(tc, 'name', 'N/A')}, "
                                      f"args={getattr(tc, 'args', 'N/A')}")
                    else:
                        print(f"      âš ï¸  NO TOOL_CALLS (assistant output without tool_calls)")
                
                # æ‰“å° generation info
                if hasattr(generation, 'generation_info') and generation.generation_info:
                    print(f"\n      Generation Info:")
                    print(f"        {json.dumps(generation.generation_info, indent=6, ensure_ascii=False)}")
        
        # æ‰“å°æµå¼æ”¶é›†çš„åŸå§‹æ–‡æœ¬
        if self.raw_response_parts:
            raw_text = ''.join(self.raw_response_parts)
            print(f"\nğŸ“„ STREAMED RAW TEXT:")
            print(f"  {raw_text[:1000]}{'...' if len(raw_text) > 1000 else ''}")
        
        self._print_separator()
    
    def on_llm_error(self, error: Exception, **kwargs: Any) -> None:
        """LLM è°ƒç”¨å‡ºé”™"""
        self._print_separator(f"âŒ LLM CALL #{self.llm_call_counter} - ERROR")
        print(f"\n  Error Type: {type(error).__name__}")
        print(f"  Error Message: {str(error)}")
        import traceback
        print(f"\n  Traceback:")
        traceback.print_exc()
        self._print_separator()
 
    # def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any) -> None:
    #     """Chain å¼€å§‹ï¼ˆå¯èƒ½åŒ…å« planning é˜¶æ®µï¼‰"""
    #
    #     # âœ… serialized å¯èƒ½ä¸º Noneï¼Œå¿…é¡»å…ˆå¤„ç†
    #     if serialized is None:
    #         chain_name = "unknown_chain"
    #     else:
    #         # âœ… serialized é‡Œå¯èƒ½æ²¡æœ‰ nameï¼Œä¹Ÿå¯èƒ½æ²¡æœ‰ idï¼Œéƒ½å¿…é¡»å®‰å…¨å¤„ç†
    #         chain_name = serialized.get("name")
    #         if chain_name is None:
    #             chain_id = serialized.get("id")
    #             if isinstance(chain_id, list) and chain_id:
    #                 chain_name = chain_id[-1]
    #             else:
    #                 chain_name = str(chain_id) if chain_id else "unknown_chain"
    #
    #     self._print_separator(f"ğŸ”— CHAIN START: {chain_name}")
    #
    #     print("\nğŸ“¥ INPUTS:")
    #
    #     # âœ… å®‰å…¨æ‰“å° inputs
    #     if inputs and 'messages' in inputs and inputs['messages']:
    #         self._print_messages(inputs['messages'], "Input Messages")
    #     else:
    #         print(json.dumps(inputs, indent=2, ensure_ascii=False))
    # 
    # def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
    #     """Chain ç»“æŸ"""
    #     if 'messages' in outputs:
    #         self._print_messages(outputs['messages'], "Output Messages")
    # 
    # def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs: Any) -> None:
    #     """Tool è°ƒç”¨å¼€å§‹"""
    #     tool_name = serialized.get('name', 'unknown')
    #     self._print_separator(f"ğŸ”§ TOOL CALL: {tool_name}")
    #     print(f"\n  Input: {input_str[:500]}")
    #     
    #     # æ‰“å° kwargs ä¸­çš„æœ‰ç”¨ä¿¡æ¯
    #     if kwargs:
    #         print(f"\n  ğŸ”§ KWARGS:")
    #         for key, value in kwargs.items():
    #             if key not in ['run_id', 'parent_run_id', 'tags', 'metadata']:
    #                 print(f"    {key}: {value}")
    # 
    # def on_tool_end(self, output: str, **kwargs: Any) -> None:
    #     """Tool è°ƒç”¨ç»“æŸ"""
    #     print(f"\n  Output: {output[:500]}{'...' if len(output) > 500 else ''}")
    #     self._print_separator()
   
    def on_tool_error(self, error: Exception, **kwargs: Any) -> None:
        """Tool è°ƒç”¨å‡ºé”™"""
        print(f"\n  âŒ Tool Error: {type(error).__name__}: {str(error)}")
        self._print_separator()
    
    def _print_messages(self, messages: List[BaseMessage], title: str = "Messages"):
        """æ‰“å°æ¶ˆæ¯åˆ—è¡¨çš„è¯¦ç»†ä¿¡æ¯"""
        print(f"\nğŸ“¨ {title} ({len(messages)} messages):")
        
        for i, msg in enumerate(messages):
            self.message_counter += 1
            msg_num = self.message_counter
            
            print(f"\n  â”Œâ”€ Message #{msg_num} (Index {i}) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
            
            # æ¶ˆæ¯ç±»å‹å’Œè§’è‰²
            msg_type = getattr(msg, 'type', None) or getattr(msg, 'role', None) or type(msg).__name__
            print(f"  â”‚ Type/Role: {msg_type}")
            
            # å†…å®¹ - æ˜¾ç¤ºå®Œæ•´å†…å®¹ï¼Œä½†æ¯è¡Œé™åˆ¶é•¿åº¦
            if hasattr(msg, 'content'):
                content = msg.content
                if content:
                    content_str = str(content)
                    # å¦‚æœå†…å®¹å¤ªé•¿ï¼Œæ˜¾ç¤ºå‰1000ä¸ªå­—ç¬¦ï¼Œå¹¶æç¤ºæ€»é•¿åº¦
                    if len(content_str) > 1000:
                        print(f"  â”‚ Content (showing first 1000 of {len(content_str)} chars):")
                        print(f"  â”‚ {content_str[:1000]}...")
                        print(f"  â”‚ ... ({len(content_str) - 1000} more characters)")
                    else:
                        # å†…å®¹ä¸å¤ªé•¿æ—¶ï¼Œå®Œæ•´æ˜¾ç¤ºï¼Œä½†æ¯è¡Œé™åˆ¶é•¿åº¦ä»¥ä¾¿é˜…è¯»
                        lines = content_str.split('\n')
                        if len(lines) == 1:
                            # å•è¡Œä½†å¯èƒ½å¾ˆé•¿
                            if len(content_str) > 500:
                                print(f"  â”‚ Content ({len(content_str)} chars, showing first 500):")
                                print(f"  â”‚ {content_str[:500]}...")
                                print(f"  â”‚ ... ({len(content_str) - 500} more characters)")
                            else:
                                print(f"  â”‚ Content: {content_str}")
                        else:
                            print(f"  â”‚ Content ({len(lines)} lines, {len(content_str)} chars):")
                            # æ˜¾ç¤ºæ‰€æœ‰è¡Œï¼Œä½†æ¯è¡Œé™åˆ¶é•¿åº¦
                            for line_idx, line in enumerate(lines[:50]):  # æœ€å¤šæ˜¾ç¤ºå‰50è¡Œ
                                if len(line) > 200:
                                    print(f"  â”‚   [{line_idx+1:3d}] {line[:200]}... ({len(line)} chars)")
                                else:
                                    print(f"  â”‚   [{line_idx+1:3d}] {line}")
                            if len(lines) > 50:
                                print(f"  â”‚   ... ({len(lines) - 50} more lines not shown)")
                else:
                    print(f"  â”‚ Content: (empty)")
            
            # tool_calls
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                print(f"  â”‚")
                print(f"  â”‚ ğŸ”§ TOOL_CALLS ({len(msg.tool_calls)}):")
                for j, tc in enumerate(msg.tool_calls):
                    if isinstance(tc, dict):
                        tc_id = tc.get('id', 'N/A')
                        tc_name = tc.get('name', 'N/A')
                        tc_args = tc.get('arguments', 'N/A')
                    else:
                        tc_id = getattr(tc, 'id', 'N/A')
                        tc_name = getattr(tc, 'name', 'N/A')
                        tc_args = getattr(tc, 'args', 'N/A')
                    
                    print(f"  â”‚   [{j}] id={tc_id}, name={tc_name}")
                    if tc_args and tc_args != 'N/A':
                        args_str = str(tc_args) if not isinstance(tc_args, dict) else json.dumps(tc_args, ensure_ascii=False)
                        print(f"  â”‚       args: {args_str[:150]}{'...' if len(args_str) > 150 else ''}")
            else:
                if msg_type in ['ai', 'assistant']:
                    print(f"  â”‚")
                    print(f"  â”‚ âš ï¸  NO TOOL_CALLS (assistant output without tool_calls)")
            
            # tool_call_id (å¯¹äº tool æ¶ˆæ¯)
            if hasattr(msg, 'tool_call_id'):
                tool_call_id = getattr(msg, 'tool_call_id', None)
                if tool_call_id:
                    print(f"  â”‚")
                    print(f"  â”‚ ğŸ”— tool_call_id: {tool_call_id}")
                else:
                    print(f"  â”‚")
                    print(f"  â”‚ âš ï¸  MISSING tool_call_id (tool message without tool_call_id)")
            
            # å…¶ä»–å±æ€§
            if hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                print(f"  â”‚")
                print(f"  â”‚ Additional kwargs: {json.dumps(msg.additional_kwargs, indent=4, ensure_ascii=False)[:200]}")
            
            print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

# ============================================================================
# Initialize Model
# ============================================================================
# Load custom prompt from markdown file
with open("src/Prompt/golf_advisor_prompt.md", "r", encoding="utf-8") as f:
    system_message = f.read()

llm = ChatOpenAI(
    model='gpt-5',
    temperature=0,  # Good: 0 for consistent, deterministic responses
    base_url=OPENAI_API_BASE,
    api_key=SecretStr(api_key) if api_key else None,
    timeout=1000
)

# æ·»åŠ  Debug Callback
debug_callback = DebugCallback()
llm = llm.with_config({"callbacks": [debug_callback]})

# ============================================================================
# âœ… PATCH: Print Final OpenAI API Payload (REAL messages sent to LLM)
# ============================================================================

from openai.resources.chat.completions import Completions
_original_create = Completions.create

def patched_create(self, *args, **kwargs):
    print("\n" + "="*80)
    print("ğŸ“¨ FINAL API PAYLOAD SENT TO OPENAI")
    print("="*80)

    try:
        import json
        print(json.dumps(kwargs, indent=2, ensure_ascii=False))
    except Exception:
        print(kwargs)

    print("="*80)
    print("âœ… END LLM PAYLOAD\n")

    return _original_create(self, *args, **kwargs)

Completions.create = patched_create



# ============================================================================
# Agent Utilities (å‚è€ƒ LangGraph 101)
# ============================================================================

def build_agent(model: ChatOpenAI):
    """Initialize LangGraph ReAct agent with custom tool and in-memory checkpointing."""
    # Create checkpointer for state management
    checkpointer = InMemorySaver()

    # ============================================================================
    # DEBUG: æ£€æŸ¥å·¥å…·æ³¨å†Œä¿¡æ¯ï¼ˆéå¸¸å…³é”®ï¼‰
    # ============================================================================
    print("=== DEBUG TOOL REGISTRATION ===")
    for tool in [retrieve_Fitting_Instructions]:
        print("Tool object:", tool)
        print("Tool name:", tool.name)
        print("Tool description:", tool.description)
        print("----")
    # ============================================================================

    # Create agent executor using LangGraph's recommended pattern
    return create_react_agent(
        model=model,
        tools=[retrieve_Fitting_Instructions],
        prompt=system_message,  # Note: create_react_agent uses 'prompt' not 'system_prompt'
        checkpointer=checkpointer
    )


def invoke_agent(agent, user_query: str, thread_id: str, callbacks: Optional[List[Any]] = None):
    """Invoke LangGraph agent with thread-aware config (mirrors langgraph_101 notebook)."""
    config: Dict[str, Any] = {"configurable": {"thread_id": thread_id}}
    if callbacks:
        config["callbacks"] = callbacks

    return agent.invoke(
        {"messages": [{"role": "user", "content": user_query}]},
        config=config
    )


def pretty_print_messages(messages: List[BaseMessage]) -> None:
    """Helper to pretty-print LangChain messages when available."""
    for message in messages:
        if hasattr(message, "pretty_print"):
            message.pretty_print()
        else:
            print(message)

# ============================================================================
# Main Execution
# ============================================================================

if __name__ == "__main__":

    print("\n" + "="*60)
    print("ğŸ” DIAGNOSTIC MODE: Tracking all API calls and timings")
    print("="*60)
    # TODO: Replace with logger.info("Starting agent query")

    # Create user query
    user_query = (
        "- I want the cheapest driver"
    )

    print(f"\nğŸ“ User query length: {len(user_query)} characters")

    # Build LangGraph agent
    agent = build_agent(llm)

    # Create config with thread_id for checkpointer
    thread_id = "993"
    print(f"ğŸ“‹ Using thread_id: {thread_id}")
    # ============================================================================
    # DIAGNOSTIC: Track total execution time
    # ============================================================================
    print(f"\nâ±ï¸  START TIME: {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
    start_time = time.time()

    print("\n" + "="*60)
    print("ğŸ“¤ Sending query to agent...")
    print("="*60)

    # ============================================================================
    # DIAGNOSTIC: Comprehensive error handling and timing
    # ============================================================================
    try:
        print("\nğŸš€ Phase 1: Invoking agent.invoke()...")
        invoke_start = time.time()

        response = invoke_agent(
            agent=agent,
            user_query=user_query,
            thread_id=thread_id,
            callbacks=[debug_callback]
        )

        invoke_end = time.time()
        invoke_duration = invoke_end - invoke_start

        print(f"\nâœ… Agent.invoke() completed successfully!")
        print(f"â±ï¸  Duration: {invoke_duration:.2f} seconds")

    except openai.APITimeoutError as e:
        end_time = time.time()
        duration = end_time - start_time
        print("\n" + "="*60)
        print("âŒ TIMEOUT ERROR DETECTED")
        print("="*60)
        print(f"â±ï¸  Total time before timeout: {duration:.2f} seconds")
        print(f"ğŸ“ Timeout setting: 80 seconds")
        print(f"ğŸ” Error details: {str(e)}")
        print("\nğŸ“Š DIAGNOSTIC ANALYSIS:")
        print(f"   - If duration â‰ˆ 80s: Agent planning or synthesis call timed out")
        print(f"   - If duration < 80s: Embedding API call may have timed out")
        print(f"   - Actual duration: {duration:.2f}s")
        print("\nğŸ’¡ Next steps:")
        print("   1. Check network connectivity to api.agicto.cn")
        print("   2. Verify model 'gpt-5-nano' is valid on your endpoint")
        print("   3. Test endpoint directly with curl/requests")
        print("="*60)
        raise

    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        print("\n" + "="*60)
        print("âŒ UNEXPECTED ERROR")
        print("="*60)
        print(f"â±ï¸  Time before error: {duration:.2f} seconds")
        print(f"ğŸ” Error type: {type(e).__name__}")
        print(f"ğŸ“ Error message: {str(e)}")
        print("="*60)
        raise

    # ============================================================================
    # DIAGNOSTIC: Track total time
    # ============================================================================
    end_time = time.time()
    total_duration = end_time - start_time
    print(f"\nâ±ï¸  END TIME: {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
    print(f"â±ï¸  TOTAL EXECUTION TIME: {total_duration:.2f} seconds")

    # TODO: Add config parameter when checkpointing is enabled

    print("\n" + "="*60)
    print("ğŸ“¤ AGENT RESPONSE")
    print("="*60)
    
    # Extract final response from agent executor output
    if isinstance(response, dict) and "messages" in response:
        pretty_print_messages(response["messages"])
        final_message = response["messages"][-1]
        if hasattr(final_message, 'content'):
            print(f"\nğŸ’¬ Final Response: {final_message.content}")
        else:
            print(f"\nğŸ’¬ Final Response: {final_message}")
    else:
        print(f"\nğŸ’¬ Response: {response}")
    
    print("="*60)
   
