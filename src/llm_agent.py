"""
LLM Agent with Tools

This module demonstrates how to create an LLM agent with custom tools using LangChain and LangGraph.
"""

import os
import time
import logging
from datetime import datetime
from dotenv import load_dotenv, find_dotenv
from langchain_openai import ChatOpenAI
from typing import Optional
from langchain.agents import create_agent
from tools import retrieve_Fitting_Instructions, retrieve_Fitted_Products
from pydantic import SecretStr
import openai
from langgraph.checkpoint.memory import InMemorySaver, MemorySaver
from langsmith import traceable
from langchain_core.messages import AIMessage, ToolMessage, BaseMessage
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from typing import Any, Dict, List, Optional
import json

# ============================================================================
# Configure Debug Logging
# ============================================================================
# logging.basicConfig(
#     level=logging.DEBUG,  # Set to DEBUG to see all message flow
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)

# ============================================================================
# IMPROVEMENT NEEDED: Add imports for memory and logging (2025 best practices)
# ============================================================================
# TODO: Add these imports:
# from langgraph.checkpoint.memory import MemorySaver  # For development
# from langgraph.checkpoint.postgres import PostgresSaver  # For production
# import logging
# from datetime import datetime
#
# # Setup logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
#     handlers=[
#         logging.FileHandler(f'logs/agent_{datetime.now().strftime("%Y%m%d")}.log'),
#         logging.StreamHandler()
#     ]
# )
# logger = logging.getLogger(__name__)
# os.makedirs('logs', exist_ok=True)
# ============================================================================

# ============================================================================
# Load environment variables
# ============================================================================

load_dotenv(find_dotenv())

api_key = os.getenv("OPENAI_API_KEY")
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE")

# ============================================================================
# IMPROVEMENT NEEDED: Replace print with logging
# ============================================================================
# TODO: Replace print statements with logger.info() for better production logging
# logger.info("OPENAI_API_KEY loaded successfully")
# logger.warning("OPENAI_API_BASE not found")
# ============================================================================

if not api_key:
    print("âŒ OPENAI_API_KEY not loaded.")
    # TODO: Consider raising an error here instead of continuing
    # raise ValueError("OPENAI_API_KEY is required but not found")
else:
    print("âœ… OPENAI_API_KEY loaded.")

if not OPENAI_API_BASE:
    print("âš ï¸ OPENAI_API_BASE not found.")
else:
    print(f"âœ… OPENAI_API_BASE = {OPENAI_API_BASE}")


# ============================================================================
# LLM Debug Callback System
# ============================================================================

class DebugCallback(BaseCallbackHandler):
    """å®Œæ•´çš„ LLM Debug Callback ç³»ç»Ÿï¼Œæ•æ‰å¹¶æ‰“å°æ‰€æœ‰å†…éƒ¨æ¶ˆæ¯"""
    
    def __init__(self):
        super().__init__()
        self.message_counter = 0
        self.llm_call_counter = 0
        self.current_prompt = None
        self.raw_response_parts = []
    
    def _print_separator(self, title: str = ""):
        """æ‰“å°åˆ†éš”çº¿"""
        if title:
            print("\n" + "="*80)
            print(f"  {title}")
            print("="*80)
        else:
            print("-"*80)
    
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        """LLM è°ƒç”¨å¼€å§‹"""
        self.llm_call_counter += 1
        self.raw_response_parts = []
        
        self._print_separator(f"ğŸš€ LLM CALL #{self.llm_call_counter} - START")
        
        # æ‰“å° prompts
        print(f"\nğŸ“ PROMPTS ({len(prompts)} prompts):")
        for i, prompt in enumerate(prompts):
            print(f"\n  Prompt {i+1}:")
            print(f"  {prompt[:500]}{'...' if len(prompt) > 500 else ''}")
        
        # æ‰“å° kwargs ä¸­çš„æœ‰ç”¨ä¿¡æ¯
        if kwargs:
            print(f"\nğŸ”§ KWARGS:")
            for key, value in kwargs.items():
                if key not in ['run_id', 'parent_run_id', 'tags', 'metadata']:
                    print(f"  {key}: {value}")
        
        self.current_prompt = prompts[0] if prompts else None
    
    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """æ¥æ”¶åˆ°æ–° tokenï¼ˆæµå¼è¾“å‡ºï¼‰"""
        self.raw_response_parts.append(token)
    
    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """LLM è°ƒç”¨ç»“æŸ"""
        self._print_separator(f"âœ… LLM CALL #{self.llm_call_counter} - END")
        
        # æ‰“å° kwargs ä¸­çš„æœ‰ç”¨ä¿¡æ¯
        if kwargs:
            print(f"\nğŸ”§ KWARGS:")
            for key, value in kwargs.items():
                if key not in ['run_id', 'parent_run_id', 'tags', 'metadata']:
                    print(f"  {key}: {value}")
        
        # æ‰“å°åŸå§‹å“åº”
        print(f"\nğŸ“¦ RAW RESPONSE:")
        if response.llm_output:
            print(f"  llm_output: {json.dumps(response.llm_output, indent=2, ensure_ascii=False)}")
        
        # æ‰“å°æ‰€æœ‰ generations
        for i, generation_list in enumerate(response.generations):
            print(f"\n  Generation Batch {i+1}:")
            for j, generation in enumerate(generation_list):
                print(f"\n    Generation {j+1}:")
                
                # æ‰“å°æ¶ˆæ¯å¯¹è±¡
                if hasattr(generation, 'message'):
                    msg = generation.message
                    print(f"      Message Type: {type(msg).__name__}")
                    print(f"      Content: {str(msg.content)[:500]}")
                    
                    # æ£€æŸ¥ tool_calls
                    if hasattr(msg, 'tool_calls') and msg.tool_calls:
                        print(f"\n      ğŸ”§ TOOL_CALLS ({len(msg.tool_calls)}):")
                        for k, tc in enumerate(msg.tool_calls):
                            if isinstance(tc, dict):
                                print(f"        [{k}] {json.dumps(tc, indent=6, ensure_ascii=False)}")
                            else:
                                print(f"        [{k}] id={getattr(tc, 'id', 'N/A')}, "
                                      f"name={getattr(tc, 'name', 'N/A')}, "
                                      f"args={getattr(tc, 'args', 'N/A')}")
                    else:
                        print(f"      âš ï¸  NO TOOL_CALLS (assistant output without tool_calls)")
                
                # æ‰“å° generation info
                if hasattr(generation, 'generation_info') and generation.generation_info:
                    print(f"\n      Generation Info:")
                    print(f"        {json.dumps(generation.generation_info, indent=6, ensure_ascii=False)}")
        
        # æ‰“å°æµå¼æ”¶é›†çš„åŸå§‹æ–‡æœ¬
        if self.raw_response_parts:
            raw_text = ''.join(self.raw_response_parts)
            print(f"\nğŸ“„ STREAMED RAW TEXT:")
            print(f"  {raw_text[:1000]}{'...' if len(raw_text) > 1000 else ''}")
        
        self._print_separator()
    
    def on_llm_error(self, error: Exception, **kwargs: Any) -> None:
        """LLM è°ƒç”¨å‡ºé”™"""
        self._print_separator(f"âŒ LLM CALL #{self.llm_call_counter} - ERROR")
        print(f"\n  Error Type: {type(error).__name__}")
        print(f"  Error Message: {str(error)}")
        import traceback
        print(f"\n  Traceback:")
        traceback.print_exc()
        self._print_separator()
 
    # def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any) -> None:
    #     """Chain å¼€å§‹ï¼ˆå¯èƒ½åŒ…å« planning é˜¶æ®µï¼‰"""
    #
    #     # âœ… serialized å¯èƒ½ä¸º Noneï¼Œå¿…é¡»å…ˆå¤„ç†
    #     if serialized is None:
    #         chain_name = "unknown_chain"
    #     else:
    #         # âœ… serialized é‡Œå¯èƒ½æ²¡æœ‰ nameï¼Œä¹Ÿå¯èƒ½æ²¡æœ‰ idï¼Œéƒ½å¿…é¡»å®‰å…¨å¤„ç†
    #         chain_name = serialized.get("name")
    #         if chain_name is None:
    #             chain_id = serialized.get("id")
    #             if isinstance(chain_id, list) and chain_id:
    #                 chain_name = chain_id[-1]
    #             else:
    #                 chain_name = str(chain_id) if chain_id else "unknown_chain"
    #
    #     self._print_separator(f"ğŸ”— CHAIN START: {chain_name}")
    #
    #     print("\nğŸ“¥ INPUTS:")
    #
    #     # âœ… å®‰å…¨æ‰“å° inputs
    #     if inputs and 'messages' in inputs and inputs['messages']:
    #         self._print_messages(inputs['messages'], "Input Messages")
    #     else:
    #         print(json.dumps(inputs, indent=2, ensure_ascii=False))
    # 
    # def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
    #     """Chain ç»“æŸ"""
    #     if 'messages' in outputs:
    #         self._print_messages(outputs['messages'], "Output Messages")
    # 
    # def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs: Any) -> None:
    #     """Tool è°ƒç”¨å¼€å§‹"""
    #     tool_name = serialized.get('name', 'unknown')
    #     self._print_separator(f"ğŸ”§ TOOL CALL: {tool_name}")
    #     print(f"\n  Input: {input_str[:500]}")
    #     
    #     # æ‰“å° kwargs ä¸­çš„æœ‰ç”¨ä¿¡æ¯
    #     if kwargs:
    #         print(f"\n  ğŸ”§ KWARGS:")
    #         for key, value in kwargs.items():
    #             if key not in ['run_id', 'parent_run_id', 'tags', 'metadata']:
    #                 print(f"    {key}: {value}")
    # 
    # def on_tool_end(self, output: str, **kwargs: Any) -> None:
    #     """Tool è°ƒç”¨ç»“æŸ"""
    #     print(f"\n  Output: {output[:500]}{'...' if len(output) > 500 else ''}")
    #     self._print_separator()
   
    def on_tool_error(self, error: Exception, **kwargs: Any) -> None:
        """Tool è°ƒç”¨å‡ºé”™"""
        print(f"\n  âŒ Tool Error: {type(error).__name__}: {str(error)}")
        self._print_separator()
    
    def _print_messages(self, messages: List[BaseMessage], title: str = "Messages"):
        """æ‰“å°æ¶ˆæ¯åˆ—è¡¨çš„è¯¦ç»†ä¿¡æ¯"""
        print(f"\nğŸ“¨ {title} ({len(messages)} messages):")
        
        for i, msg in enumerate(messages):
            self.message_counter += 1
            msg_num = self.message_counter
            
            print(f"\n  â”Œâ”€ Message #{msg_num} (Index {i}) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
            
            # æ¶ˆæ¯ç±»å‹å’Œè§’è‰²
            msg_type = getattr(msg, 'type', None) or getattr(msg, 'role', None) or type(msg).__name__
            print(f"  â”‚ Type/Role: {msg_type}")
            
            # å†…å®¹ - æ˜¾ç¤ºå®Œæ•´å†…å®¹ï¼Œä½†æ¯è¡Œé™åˆ¶é•¿åº¦
            if hasattr(msg, 'content'):
                content = msg.content
                if content:
                    content_str = str(content)
                    # å¦‚æœå†…å®¹å¤ªé•¿ï¼Œæ˜¾ç¤ºå‰1000ä¸ªå­—ç¬¦ï¼Œå¹¶æç¤ºæ€»é•¿åº¦
                    if len(content_str) > 1000:
                        print(f"  â”‚ Content (showing first 1000 of {len(content_str)} chars):")
                        print(f"  â”‚ {content_str[:1000]}...")
                        print(f"  â”‚ ... ({len(content_str) - 1000} more characters)")
                    else:
                        # å†…å®¹ä¸å¤ªé•¿æ—¶ï¼Œå®Œæ•´æ˜¾ç¤ºï¼Œä½†æ¯è¡Œé™åˆ¶é•¿åº¦ä»¥ä¾¿é˜…è¯»
                        lines = content_str.split('\n')
                        if len(lines) == 1:
                            # å•è¡Œä½†å¯èƒ½å¾ˆé•¿
                            if len(content_str) > 500:
                                print(f"  â”‚ Content ({len(content_str)} chars, showing first 500):")
                                print(f"  â”‚ {content_str[:500]}...")
                                print(f"  â”‚ ... ({len(content_str) - 500} more characters)")
                            else:
                                print(f"  â”‚ Content: {content_str}")
                        else:
                            print(f"  â”‚ Content ({len(lines)} lines, {len(content_str)} chars):")
                            # æ˜¾ç¤ºæ‰€æœ‰è¡Œï¼Œä½†æ¯è¡Œé™åˆ¶é•¿åº¦
                            for line_idx, line in enumerate(lines[:50]):  # æœ€å¤šæ˜¾ç¤ºå‰50è¡Œ
                                if len(line) > 200:
                                    print(f"  â”‚   [{line_idx+1:3d}] {line[:200]}... ({len(line)} chars)")
                                else:
                                    print(f"  â”‚   [{line_idx+1:3d}] {line}")
                            if len(lines) > 50:
                                print(f"  â”‚   ... ({len(lines) - 50} more lines not shown)")
                else:
                    print(f"  â”‚ Content: (empty)")
            
            # tool_calls
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                print(f"  â”‚")
                print(f"  â”‚ ğŸ”§ TOOL_CALLS ({len(msg.tool_calls)}):")
                for j, tc in enumerate(msg.tool_calls):
                    if isinstance(tc, dict):
                        tc_id = tc.get('id', 'N/A')
                        tc_name = tc.get('name', 'N/A')
                        tc_args = tc.get('arguments', 'N/A')
                    else:
                        tc_id = getattr(tc, 'id', 'N/A')
                        tc_name = getattr(tc, 'name', 'N/A')
                        tc_args = getattr(tc, 'args', 'N/A')
                    
                    print(f"  â”‚   [{j}] id={tc_id}, name={tc_name}")
                    if tc_args and tc_args != 'N/A':
                        args_str = str(tc_args) if not isinstance(tc_args, dict) else json.dumps(tc_args, ensure_ascii=False)
                        print(f"  â”‚       args: {args_str[:150]}{'...' if len(args_str) > 150 else ''}")
            else:
                if msg_type in ['ai', 'assistant']:
                    print(f"  â”‚")
                    print(f"  â”‚ âš ï¸  NO TOOL_CALLS (assistant output without tool_calls)")
            
            # tool_call_id (å¯¹äº tool æ¶ˆæ¯)
            if hasattr(msg, 'tool_call_id'):
                tool_call_id = getattr(msg, 'tool_call_id', None)
                if tool_call_id:
                    print(f"  â”‚")
                    print(f"  â”‚ ğŸ”— tool_call_id: {tool_call_id}")
                else:
                    print(f"  â”‚")
                    print(f"  â”‚ âš ï¸  MISSING tool_call_id (tool message without tool_call_id)")
            
            # å…¶ä»–å±æ€§
            if hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                print(f"  â”‚")
                print(f"  â”‚ Additional kwargs: {json.dumps(msg.additional_kwargs, indent=4, ensure_ascii=False)[:200]}")
            
            print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

# ============================================================================
# Initialize Model
# ============================================================================

# ============================================================================
# IMPROVEMENT NEEDED: Add error handling and configuration
# ============================================================================
# TODO: Add error handling for model initialization
# TODO: Add timeout parameter (timeout=30.0)
# TODO: Consider moving config to separate config file
# ============================================================================

# Load custom prompt from markdown file
with open("src/Prompt/golf_advisor_prompt.md", "r", encoding="utf-8") as f:
    system_message = f.read()

llm = ChatOpenAI(
    model='gpt-5',
    temperature=0,  # Good: 0 for consistent, deterministic responses
    base_url=OPENAI_API_BASE,
    api_key=SecretStr(api_key) if api_key else None,
    timeout=1000
)

# æ·»åŠ  Debug Callback
debug_callback = DebugCallback()
llm = llm.with_config({"callbacks": [debug_callback]})

# ============================================================================
# âœ… PATCH: Print Final OpenAI API Payload (REAL messages sent to LLM)
# ============================================================================

from openai.resources.chat.completions import Completions
_original_create = Completions.create

def patched_create(self, *args, **kwargs):
    print("\n" + "="*80)
    print("ğŸ“¨ FINAL API PAYLOAD SENT TO OPENAI")
    print("="*80)

    try:
        import json
        print(json.dumps(kwargs, indent=2, ensure_ascii=False))
    except Exception:
        print(kwargs)

    print("="*80)
    print("âœ… END LLM PAYLOAD\n")

    return _original_create(self, *args, **kwargs)

Completions.create = patched_create

# ============================================================================
# Agent
# ============================================================================

# ============================================================================
# IMPROVEMENT NEEDED: Add error handling for file read
# ============================================================================
# TODO: Add try-except for file reading
# try:
#     with open("src/Prompt/golf_advisor_prompt.md", "r", encoding="utf-8") as f:
#         system_message = f.read()
# except FileNotFoundError:
#     logger.error("Prompt file not found")
#     raise
# ============================================================================

# ============================================================================

# ============================================================================
# ğŸ”´ CRITICAL LIMITATION: Memory disabled!
# ============================================================================
# checkpointer=False means NO conversation memory!
#
# This causes:
# - Agent forgets previous messages in same conversation
# - Can't do multi-turn conversations
# - No error recovery (can't resume from failures)
# - Missing human-in-the-loop capabilities
# - No state inspection for debugging
#
# The comment says "to avoid message ordering issues" but this is likely
# a misunderstanding. Checkpointing is STABLE in LangGraph 2025.
#
# BENEFITS of enabling checkpointing (2025 research):
# âœ… Conversation memory across turns
# âœ… Error recovery (restart from last checkpoint)
# âœ… Human-in-the-loop workflows
# âœ… State inspection for debugging
# âœ… Fault tolerance
#
# TODO: Enable checkpointing:
# 1. Import: from langgraph.checkpoint.memory import MemorySaver
# 2. Create: checkpointer = MemorySaver()
# 3. Use: checkpointer=checkpointer (replace False)
# 4. When invoking, add config with thread_id:
#    config = {"configurable": {"thread_id": "session_001"}}
#    response = agent.invoke({"messages": [...]}, config=config)
#
# For production, use PostgresSaver instead of MemorySaver
# ============================================================================

# Create checkpointer for state management
checkpointer = MemorySaver()
checkpointer.storage.clear()   # Ste å¼ºåˆ¶æ¸…ç©ºæ—§ memory

# Create agent with prompt parameter
# Note: Checkpointer ENABLED to maintain proper message history for multi-tool calls

# ============================================================================
# DEBUG: æ£€æŸ¥å·¥å…·æ³¨å†Œä¿¡æ¯ï¼ˆéå¸¸å…³é”®ï¼‰
# ============================================================================
print("=== DEBUG TOOL REGISTRATION ===")
for tool in [retrieve_Fitting_Instructions, retrieve_Fitted_Products]:
    print("Tool object:", tool)
    print("Tool name:", tool.name)
    print("Tool description:", tool.description)
    print("----")
# ============================================================================

# Create agent executor using LangGraph's recommended pattern
agent = create_agent(
    model=llm,
    tools=[retrieve_Fitting_Instructions],
    system_prompt=system_message,  # Note: create_react_agent uses 'prompt' not 'system_prompt'
    checkpointer=checkpointer
)

# ============================================================================
# Main Execution
# ============================================================================

if __name__ == "__main__":
    # ============================================================================
    # IMPROVEMENT NEEDED: Add comprehensive error handling (2025 best practice)
    # ============================================================================
    # TODO: Wrap entire execution in try-except block
    # TODO: Add logging instead of print statements
    # TODO: Log execution time and token usage
    # ============================================================================

    print("\n" + "="*60)
    print("ğŸ” DIAGNOSTIC MODE: Tracking all API calls and timings")
    print("="*60)
    # TODO: Replace with logger.info("Starting agent query")

    # Create user query
    user_query = (
        "- Driver Swing Speed: Average 121.5 mph, Peak 124.4 mph\n"
        "- Ball Speed: Up to 180 mph, notable 186 mph\n"
        "- Height: 6 feet 1 inch (185 cm)\n"
        "- Weight: 185 pounds (84 kg)\n"
        "- Age: 49 years old\n"
        "- What should my driver be like?\n"
        "- What Driver should I get?"
    )

    print(f"\nğŸ“ User query length: {len(user_query)} characters")

    # Create config with thread_id for checkpointer
    thread_id = "937"
    print(f"ğŸ“‹ Using thread_id: {thread_id}")

    # ============================================================================
    # DIAGNOSTIC: Track total execution time
    # ============================================================================
    print(f"\nâ±ï¸  START TIME: {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
    start_time = time.time()

    print("\n" + "="*60)
    print("ğŸ“¤ Sending query to agent...")
    print("="*60)

    # ============================================================================
    # DIAGNOSTIC: Comprehensive error handling and timing
    # ============================================================================
    try:
        print("\nğŸš€ Phase 1: Invoking agent.invoke()...")
        invoke_start = time.time()

        response = agent.invoke(
            {
                "messages": [
                    {"role": "user", "content": user_query} 
                ]
            },
            config={
                "configurable": {"thread_id": thread_id},
                "callbacks": [debug_callback]  # âœ… âœ… âœ… å…³é”®
            }  # Ste å¿…é¡»ä½¿ç”¨configæ¥ä¼ é€’thread_idå’Œdebug_callback
        )

        invoke_end = time.time()
        invoke_duration = invoke_end - invoke_start

        print(f"\nâœ… Agent.invoke() completed successfully!")
        print(f"â±ï¸  Duration: {invoke_duration:.2f} seconds")

    except openai.APITimeoutError as e:
        end_time = time.time()
        duration = end_time - start_time
        print("\n" + "="*60)
        print("âŒ TIMEOUT ERROR DETECTED")
        print("="*60)
        print(f"â±ï¸  Total time before timeout: {duration:.2f} seconds")
        print(f"ğŸ“ Timeout setting: 80 seconds")
        print(f"ğŸ” Error details: {str(e)}")
        print("\nğŸ“Š DIAGNOSTIC ANALYSIS:")
        print(f"   - If duration â‰ˆ 80s: Agent planning or synthesis call timed out")
        print(f"   - If duration < 80s: Embedding API call may have timed out")
        print(f"   - Actual duration: {duration:.2f}s")
        print("\nğŸ’¡ Next steps:")
        print("   1. Check network connectivity to api.agicto.cn")
        print("   2. Verify model 'gpt-5-nano' is valid on your endpoint")
        print("   3. Test endpoint directly with curl/requests")
        print("="*60)
        raise

    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        print("\n" + "="*60)
        print("âŒ UNEXPECTED ERROR")
        print("="*60)
        print(f"â±ï¸  Time before error: {duration:.2f} seconds")
        print(f"ğŸ” Error type: {type(e).__name__}")
        print(f"ğŸ“ Error message: {str(e)}")
        print("="*60)
        raise

    # ============================================================================
    # DIAGNOSTIC: Track total time
    # ============================================================================
    end_time = time.time()
    total_duration = end_time - start_time
    print(f"\nâ±ï¸  END TIME: {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
    print(f"â±ï¸  TOTAL EXECUTION TIME: {total_duration:.2f} seconds")

    # TODO: Add config parameter when checkpointing is enabled

    print("\n" + "="*60)
    print("ğŸ“¤ AGENT RESPONSE")
    print("="*60)
    
    # Extract final response from agent executor output
    if isinstance(response, dict):
        if "messages" in response:
            final_message = response["messages"][-1]
            if hasattr(final_message, 'content'):
                print(f"\nğŸ’¬ Final Response: {final_message.content}")
            else:
                print(f"\nğŸ’¬ Final Response: {final_message}")
        else:
            print(f"\nğŸ’¬ Response: {response}")
    else:
        print(f"\nğŸ’¬ Response: {response}")
    
    print("="*60)
   

    # ============================================================================
    # IMPROVEMENT NEEDED: Better response parsing and error handling
    # ============================================================================
    # TODO: Add validation that response is not None
    # TODO: Add error handling for malformed responses
    # TODO: Log tool calls for debugging
    # ============================================================================

    

    # ============================================================================
    # IMPROVEMENT NEEDED: Add example of multi-turn conversation
    # ============================================================================
    # TODO: Show how to continue conversation with same thread_id:
    # # Follow-up question
    # follow_up = "What about the shaft flex?"
    # response2 = agent.invoke(
    #     {"messages": [("user", follow_up)]},
    #     config={"configurable": {"thread_id": "session_001"}}  # Same thread_id
    # )
    # # Agent will remember previous context!
    # ============================================================================
   